{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pylab\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Inverse Reinforcement Learning\n",
    "\n",
    "The Mountain Car problem is \"on a one-dimensional track, positioned between two ‚Äúmountains‚Äù. The goal is to drive up the mountain on the right; however, the car‚Äôs engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum.\"\n",
    "\n",
    "<img src='https://miro.medium.com/max/1104/1*JjBfoFrKCoBxlraVZaEshw.jpeg'>\n",
    "\n",
    "The car‚Äôs state, at any point in time, is given by a vector containing its horizonal position and velocity. The car commences each episode stationary, at the bottom of the valley between the hills (at position, x, approximately -0.5), and the episode ends when either the car reaches the flag (position > 0.5) or after 200 moves.\n",
    "\n",
    "Your state, s, is a 2-dim vector that will be (position, velocity) or (x,v) of (-0.5,0) in the beginning because the velocity is 0 and x = -0.5 is the valley between the two mountains which is left of center (there is about a distance of 1.1 in front of you and a distance of -0.7 behind you). Only the x-axis is part of the state, the elevation (y-axis) and gravity is implicit. \n",
    "\n",
    "The velocity is positive is you are moving forward to the right, and negative if you are moving to the left. \n",
    "\n",
    "action 0 is do nothing, action 1 is run the car in reverse towards the left mountain located at `state[0]=-1.2`,\n",
    "action 2 is run the car forward towards `state[0]=0.6`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MACROS\n",
    "Push_Left = 0\n",
    "No_Push = 1\n",
    "Push_Right = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-4.76850926e-01, -3.52075133e-04]), -1.0, False, {})\n",
      "(array([-0.48371095, -0.00202197]), -1.0, False, {})\n",
      "(array([-0.49792094, -0.00332095]), -1.0, False, {})\n",
      "(array([-0.51686825, -0.00400851]), -1.0, False, {})\n",
      "(array([-0.53705828, -0.00395614]), -1.0, False, {})\n",
      "(array([-0.5547636 , -0.00317364]), -1.0, False, {})\n",
      "(array([-0.56672165, -0.00180714]), -1.0, False, {})\n",
      "(array([-5.70736411e-01, -1.09287782e-04]), -1.0, False, {})\n",
      "(array([-0.56607247,  0.00160857]), -1.0, False, {})\n",
      "(array([-0.55358426,  0.00303159]), -1.0, False, {})\n",
      "(array([-0.53556574,  0.003897  ]), -1.0, False, {})\n",
      "(array([-0.51533794,  0.00404339]), -1.0, False, {})\n",
      "(array([-0.49663536,  0.00344328]), -1.0, False, {})\n",
      "(array([-0.48290684,  0.00220896]), -1.0, False, {})\n",
      "(array([-0.47667582,  0.00056926]), -1.0, False, {})\n",
      "(array([-0.4790842 , -0.00117466]), -1.0, False, {})\n",
      "(array([-0.48969092, -0.0027034 ]), -1.0, False, {})\n",
      "(array([-0.50654931, -0.0037354 ]), -1.0, False, {})\n",
      "(array([-0.52655467, -0.00407862]), -1.0, False, {})\n",
      "(array([-0.54601424, -0.00366874]), -1.0, False, {})\n"
     ]
    }
   ],
   "source": [
    "# Initialize environment and reset it to start at valley with 0 velocity\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.reset() # array([-0.44943939,  0.        ])\n",
    "\n",
    "# experiment with constantly moving in one direction or the other to see what happens\n",
    "const_action = No_Push\n",
    "for t in range(100):\n",
    "    update = env.step(const_action)\n",
    "    if t % 5 == 0:\n",
    "        print(update)\n",
    "    \n",
    "# if you do nothing you jus swing side to side, but your position stays in (-0.64,-0.4)\n",
    "# your velocity stays in (-0.01,+0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-5.53310824e-01,  2.24220318e-04]), -1.0, False, {})\n",
      "(array([-0.54894264,  0.00128738]), -1.0, False, {})\n",
      "(array([-0.53989785,  0.00211342]), -1.0, False, {})\n",
      "(array([-0.52784398,  0.0025496 ]), -1.0, False, {})\n",
      "(array([-0.51500612,  0.00251499]), -1.0, False, {})\n",
      "(array([-0.50375501,  0.002016  ]), -1.0, False, {})\n",
      "(array([-0.49616677,  0.00114521]), -1.0, False, {})\n",
      "(array([-4.93639728e-01,  6.35227098e-05]), -1.0, False, {})\n",
      "(array([-0.49663907, -0.00102986]), -1.0, False, {})\n",
      "(array([-0.50461262, -0.0019336 ]), -1.0, False, {})\n",
      "(array([-0.51609091, -0.00248081]), -1.0, False, {})\n",
      "(array([-0.52895565, -0.00256999]), -1.0, False, {})\n",
      "(array([-0.54083112, -0.00218458]), -1.0, False, {})\n",
      "(array([-0.54952534, -0.00139613]), -1.0, False, {})\n",
      "(array([-5.53435615e-01, -3.50484997e-04]), -1.0, False, {})\n",
      "(array([-0.551842  ,  0.00075966]), -1.0, False, {})\n",
      "(array([-0.54503784,  0.00172997]), -1.0, False, {})\n",
      "(array([-0.53427667,  0.0023814 ]), -1.0, False, {})\n",
      "(array([-0.52154378,  0.00259328]), -1.0, False, {})\n",
      "(array([-0.50919042,  0.00232623]), -1.0, False, {})\n"
     ]
    }
   ],
   "source": [
    "# Initialize environment and reset it to start at valley with 0 velocity\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.reset() # array([-0.44943939,  0.        ])\n",
    "\n",
    "# experiment with constantly moving in one direction or the other to see what happens\n",
    "# constantly do nothing\n",
    "const_action = No_Push\n",
    "for t in range(100):\n",
    "    update = env.step(const_action)\n",
    "    if t % 5 == 0:\n",
    "        print(update)\n",
    "    \n",
    "# if you do nothing you jus swing side to side, but your position stays in (-0.64,-0.4)\n",
    "# your velocity stays in (-0.01,+0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-4.02942532e-01,  1.15520119e-04]), -1.0, False, {})\n",
      "(array([-0.40068835,  0.00066509]), -1.0, False, {})\n",
      "(array([-0.39599541,  0.0011    ]), -1.0, False, {})\n",
      "(array([-0.38967038,  0.00134621]), -1.0, False, {})\n",
      "(array([-0.38279361,  0.00136316]), -1.0, False, {})\n",
      "(array([-0.37653034,  0.00114992]), -1.0, False, {})\n",
      "(array([-0.37193303,  0.00074409]), -1.0, False, {})\n",
      "(array([-3.69768642e-01,  2.14460993e-04]), -1.0, False, {})\n",
      "(array([-3.70396588e-01, -3.50736679e-04]), -1.0, False, {})\n",
      "(array([-0.3737127 , -0.00085774]), -1.0, False, {})\n",
      "(array([-0.37916541, -0.00122184]), -1.0, False, {})\n",
      "(array([-0.38584234, -0.00138091]), -1.0, False, {})\n",
      "(array([-0.39261739, -0.00130631]), -1.0, False, {})\n",
      "(array([-0.39833835, -0.00100881]), -1.0, False, {})\n",
      "(array([-0.40202502, -0.000538  ]), -1.0, False, {})\n",
      "(array([-4.03042392e-01,  2.56395926e-05]), -1.0, False, {})\n",
      "(array([-0.40121477,  0.00058485]), -1.0, False, {})\n",
      "(array([-0.39685764,  0.00104317]), -1.0, False, {})\n",
      "(array([-0.39072054,  0.00132237]), -1.0, False, {})\n",
      "(array([-0.38385288,  0.0013761 ]), -1.0, False, {})\n"
     ]
    }
   ],
   "source": [
    "# Initialize environment and reset it to start at valley with 0 velocity\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.reset() # array([-0.44943939,  0.        ])\n",
    "\n",
    "# experiment with constantly moving in one direction or the other to see what happens\n",
    "# constantly go right\n",
    "const_action = Push_Right\n",
    "for t in range(100):\n",
    "    update = env.step(const_action)\n",
    "    if t % 5 == 0:\n",
    "        print(update)\n",
    "    \n",
    "# if you constantly go right,  your position stays in (-0.52,-0.24)\n",
    "# your velocity stays in (-0.011,+0.011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-0.52770694, -0.00097651]), -1.0, False, {})\n",
      "(array([-0.54672927, -0.0056059 ]), -1.0, False, {})\n",
      "(array([-0.58611381, -0.00920342]), -1.0, False, {})\n",
      "(array([-0.63866887, -0.0111318 ]), -1.0, False, {})\n",
      "(array([-0.69506712, -0.01111902]), -1.0, False, {})\n",
      "(array([-0.74585421, -0.00929901]), -1.0, False, {})\n",
      "(array([-0.783204  , -0.00610299]), -1.0, False, {})\n",
      "(array([-0.80187871, -0.0020843 ]), -1.0, False, {})\n",
      "(array([-0.7994467 ,  0.00219923]), -1.0, False, {})\n",
      "(array([-0.77621717,  0.00620306]), -1.0, False, {})\n",
      "(array([-0.73525548,  0.00936888]), -1.0, False, {})\n",
      "(array([-0.68242697,  0.01114462]), -1.0, False, {})\n",
      "(array([-0.62604412,  0.01110483]), -1.0, False, {})\n",
      "(array([-0.57570754,  0.00912572]), -1.0, False, {})\n",
      "(array([-0.54042938,  0.00549046]), -1.0, False, {})\n",
      "(array([-0.52667313,  0.00084439]), -1.0, False, {})\n",
      "(array([-0.53697607, -0.00395756]), -1.0, False, {})\n",
      "(array([-0.56943731, -0.0080297 ]), -1.0, False, {})\n",
      "(array([-0.61809934, -0.01063427]), -1.0, False, {})\n",
      "(array([-0.67420743, -0.01135044]), -1.0, False, {})\n"
     ]
    }
   ],
   "source": [
    "# Initialize environment and reset it to start at valley with 0 velocity\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.reset() # array([-0.44943939,  0.        ])\n",
    "\n",
    "# experiment with constantly moving in one direction or the other to see what happens\n",
    "# constantly go left\n",
    "const_action = Push_Left\n",
    "for t in range(100):\n",
    "    update = env.step(const_action)\n",
    "    if t % 5 == 0:\n",
    "        print(update)\n",
    "    \n",
    "# if you constantly go left,  your position stays in (-0.85,-0.48)\n",
    "# your velocity stays in (-0.015,+0.011)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the update, the tuple  (array([-0.66511328, -0.01531442]), -1.0, False, {})\n",
    " contains the (state, reward, end_of_episode_bool, meta_data)\n",
    "the meta_data is empty for this environment, there is a negative reward for every timestep that \n",
    " you have not reached the goal yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 130, 3)\n",
      "[-0.90691623 -0.02983074  0.        ] (3,)\n"
     ]
    }
   ],
   "source": [
    "# load the expert 20 demonstrations\n",
    "expert_demo = np.load('/shared/Carson/RL/lets-do-irl/mountaincar/maxent/expert_demo/expert_demo.npy')\n",
    "print(expert_demo.shape)\n",
    "# (number of demonstrations, length of demonstrations, states and actions of demonstrations)\n",
    "print(expert_demo[0,60,:], expert_demo[0,0,:].shape)\n",
    "# as you can see from step 60 of the first example, the best strategy is to first accelerate backwards into\n",
    "# the < -0.8 range into order to gain speed going right into the valley. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.2  -0.07] [0.6  0.07] [0.09  0.007] 1.8000000715255737 0.14000000432133675\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "To descretize the state space, we separate the range of possible continuous positions x and\n",
    "continuous velocities y into 20 bins (one_feature)\n",
    "'''\n",
    "\n",
    "one_feature = 20 # number of state per one feature\n",
    "\n",
    "env_low = env.observation_space.low     \n",
    "env_high = env.observation_space.high   \n",
    "env_distance = (env_high - env_low) / one_feature  \n",
    "\n",
    "print(env_low, env_high, env_distance, env_distance[0]*20, env_distance[1]*20)\n",
    "# the range of x is 1.8 and velocity is 0.14 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "using this bin size we descretize the expert demonstrations\n",
    "The difference between expert_demo and demonstrations is that\n",
    "demonstrations consists of discreet integers instead of continuous values\n",
    "for it's states\n",
    "\n",
    "\n",
    "\n",
    "here I used n for sample index and t for timestep index\n",
    "'''\n",
    "def idx_state(state):\n",
    "    '''\n",
    "    this function converts a continuous state vector of\n",
    "    2-dim into a discrete index\n",
    "    \n",
    "    by assigning each state and index of state_idx = position_idx + velocity_idx * one_feature\n",
    "    we make sure that (position_idx, velocity_idx) = (2,3) and (3,2) map to different integers\n",
    "    The first 20 elements of state_idx go to position_idx = 0 - 19, velocity_idx = 0, etc\n",
    "    '''\n",
    "    position_idx = int((state[0] - env_low[0]) / env_distance[0])\n",
    "    velocity_idx = int((state[1] - env_low[1]) / env_distance[1])\n",
    "    state_idx = position_idx + velocity_idx * one_feature\n",
    "    return state_idx\n",
    "\n",
    "demonstrations = np.zeros((len(expert_demo), len(expert_demo[0]), 3))\n",
    "\n",
    "for n in range(len(expert_demo)):\n",
    "    \n",
    "    for t in range(len(expert_demo[0])):\n",
    "\n",
    "        state_idx = idx_state(expert_demo[n][t])\n",
    "\n",
    "        demonstrations[n][t][0] = state_idx\n",
    "        demonstrations[n][t][1] = expert_demo[n][t][2] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning is a ‚Äúmodel-free, off-policy‚Äù RL algorithm.\n",
    "\n",
    "‚Äúmodel-free‚Äù because we are not trying to build a pre-determined model of our environment (which may or may not be accurate)\n",
    "\n",
    "‚Äúoff-policy‚Äù because our behavior in interacting with (or exploring) the environment may be unrelated to what we believe to be the best optimal at the time of taking the action.\n",
    "\n",
    "You might think of Q-learning as updating a table where each row is a different state and each column is an action that can be taken in that state and where they intersect is the Q-value for that state action pair. the Q-table maps various (state, action) pairs with the Q-value (the expected sum of discounter future rewards when taking action a in state s and behaving optimally thereafter)\n",
    "\n",
    "It doesnt have to be a square table like this though, Q just has to map some state or state-action to some Q-value\n",
    "\n",
    "In the above example you can see that the pattern is that we observe some state, s, take an action a, and receive a new state s‚Äô, then repeat. \n",
    "\n",
    "The update rule is the weighted average of the old Q(s,a) value and the Q value implied by the new observation. That is, the sum of (i) the immediate reward and (ii) the expected discounted reward received from the new state onwards, assuming you always choose the optimal action.\n",
    "\n",
    "Q‚Äô(s,a) = (1 ‚Äî alpha) * Q(s, a) + alpha *(r + gamma * Q(s‚Äô, argmax a‚Äô : Q(s‚Äô, a‚Äô)))\n",
    "\n",
    "where argmax a‚Äô : Q(s‚Äô, a‚Äô) is the action  a‚Äô that has the highest current Q value among our row of states s‚Äô\n",
    "\n",
    "This equation for update is equivalently written \n",
    "\n",
    "ùëÑ(ùë†,ùëé)‚ÜêùëÑ(ùë†,ùëé)+ùõº(ùëü+ùõæ*max_ùëé‚Ä≤ùëÑ(ùë†‚Ä≤,ùëé‚Ä≤)‚àíùëÑ(ùë†,ùëé)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_table(state, action, reward, next_state):\n",
    "    ''' \n",
    "    The Q-learning update rule\n",
    "    ùëÑ(ùë†,ùëé)‚ÜêùëÑ(ùë†,ùëé)+ùõº(ùëü+ùõæ*max_ùëé‚Ä≤ùëÑ(ùë†‚Ä≤,ùëé‚Ä≤)‚àíùëÑ(ùë†,ùëé))\n",
    "    '''\n",
    "    q_1 = q_table[state][action]\n",
    "    q_2 = reward + gamma * max(q_table[next_state])\n",
    "    q_table[state][action] += q_learning_rate * (q_2 - q_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
