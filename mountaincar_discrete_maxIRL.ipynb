{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "%pylab inline\n",
    "import pylab\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Maximum Entropy Inverse Reinforcement Learning (MaxEntIRL)\n",
    "\n",
    "The Mountain Car problem is \"on a one-dimensional track, positioned between two ‚Äúmountains‚Äù. The goal is to drive up the mountain on the right; however, the car‚Äôs engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum.\"\n",
    "\n",
    "<img src='https://miro.medium.com/max/1104/1*JjBfoFrKCoBxlraVZaEshw.jpeg'>\n",
    "\n",
    "The car‚Äôs state, at any point in time, is given by a vector containing its horizonal position and velocity. The car commences each episode stationary, at the bottom of the valley between the hills (at position, x, approximately -0.5), and the episode ends when either the car reaches the flag (position > 0.5) or after 200 moves.\n",
    "\n",
    "Your state, s, is a 2-dim vector that will be (position, velocity) or (x,v) of (-0.5,0) in the beginning because the velocity is 0 and x = -0.5 is the valley between the two mountains which is left of center (there is about a distance of 1.1 in front of you and a distance of -0.7 behind you). Only the x-axis is part of the state, the elevation (y-axis) and gravity is implicit. \n",
    "\n",
    "The velocity is positive is you are moving forward to the right, and negative if you are moving to the left. \n",
    "\n",
    "action 0 is do nothing, action 1 is run the car in reverse towards the left mountain located at `state[0]=-1.2`,\n",
    "action 2 is run the car forward towards `state[0]=0.6`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MACROS\n",
    "Push_Left = 0\n",
    "No_Push = 1\n",
    "Push_Right = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-5.52728967e-01,  2.19840917e-04]), -1.0, False, {})\n",
      "(array([-0.54844612,  0.00126223]), -1.0, False, {})\n",
      "(array([-0.53957812,  0.0020721 ]), -1.0, False, {})\n",
      "(array([-0.52776001,  0.00249972]), -1.0, False, {})\n",
      "(array([-0.51517341,  0.00246575]), -1.0, False, {})\n",
      "(array([-0.50414268,  0.00197649]), -1.0, False, {})\n",
      "(array([-0.49670336,  0.0011227 ]), -1.0, False, {})\n",
      "(array([-4.94226444e-01,  6.21254728e-05]), -1.0, False, {})\n",
      "(array([-0.49716795, -0.00100988]), -1.0, False, {})\n",
      "(array([-0.50498629, -0.00189591]), -1.0, False, {})\n",
      "(array([-0.51624046, -0.00243232]), -1.0, False, {})\n",
      "(array([-0.52885349, -0.00251966]), -1.0, False, {})\n",
      "(array([-0.5404961, -0.0021417]), -1.0, False, {})\n",
      "(array([-0.54901924, -0.00136859]), -1.0, False, {})\n",
      "(array([-5.52851654e-01, -3.43329022e-04]), -1.0, False, {})\n",
      "(array([-0.55128763,  0.00074513]), -1.0, False, {})\n",
      "(array([-0.5446151 ,  0.00169639]), -1.0, False, {})\n",
      "(array([-0.53406347,  0.00233495]), -1.0, False, {})\n",
      "(array([-0.52157949,  0.00254252]), -1.0, False, {})\n",
      "(array([-0.50946842,  0.00228053]), -1.0, False, {})\n"
     ]
    }
   ],
   "source": [
    "# Initialize environment and reset it to start at valley with 0 velocity\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.reset() # array([-0.44943939,  0.        ])\n",
    "\n",
    "# experiment with constantly moving in one direction or the other to see what happens\n",
    "const_action = No_Push\n",
    "for t in range(100):\n",
    "    update = env.step(const_action)\n",
    "    if t % 5 == 0:\n",
    "        print(update)\n",
    "    \n",
    "# if you do nothing you jus swing side to side, but your position stays in (-0.64,-0.4)\n",
    "# your velocity stays in (-0.01,+0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-0.43772489, -0.00064161]), -1.0, False, {})\n",
      "(array([-0.45023362, -0.00368832]), -1.0, False, {})\n",
      "(array([-0.476185 , -0.0060695]), -1.0, False, {})\n",
      "(array([-0.51085975, -0.00734188]), -1.0, False, {})\n",
      "(array([-0.54788467, -0.00726095]), -1.0, False, {})\n",
      "(array([-0.58043168, -0.00584245]), -1.0, False, {})\n",
      "(array([-0.6025358 , -0.00335815]), -1.0, False, {})\n",
      "(array([-6.10192431e-01, -2.68653394e-04]), -1.0, False, {})\n",
      "(array([-0.60202659,  0.00286897]), -1.0, False, {})\n",
      "(array([-0.57950494,  0.0054904 ]), -1.0, False, {})\n",
      "(array([-0.54670929,  0.00711236]), -1.0, False, {})\n",
      "(array([-0.50965213,  0.00742559]), -1.0, False, {})\n",
      "(array([-0.47516783,  0.00636919]), -1.0, False, {})\n",
      "(array([-0.4495929 ,  0.00414696]), -1.0, False, {})\n",
      "(array([-0.43757634,  0.001175  ]), -1.0, False, {})\n",
      "(array([-0.44127993, -0.0020076 ]), -1.0, False, {})\n",
      "(array([-0.46003917, -0.00482979]), -1.0, False, {})\n",
      "(array([-0.49046494, -0.00677595]), -1.0, False, {})\n",
      "(array([-0.52699596, -0.00747833]), -1.0, False, {})\n",
      "(array([-0.56290013, -0.00680079]), -1.0, False, {})\n"
     ]
    }
   ],
   "source": [
    "# Initialize environment and reset it to start at valley with 0 velocity\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.reset() # array([-0.44943939,  0.        ])\n",
    "\n",
    "# experiment with constantly moving in one direction or the other to see what happens\n",
    "# constantly do nothing\n",
    "const_action = No_Push\n",
    "for t in range(100):\n",
    "    update = env.step(const_action)\n",
    "    if t % 5 == 0:\n",
    "        print(update)\n",
    "    \n",
    "# if you do nothing you jus swing side to side, but your position stays in (-0.64,-0.4)\n",
    "# your velocity stays in (-0.01,+0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-0.58871941,  0.00149631]), -1.0, False, {})\n",
      "(array([-0.55955899,  0.00859576]), -1.0, False, {})\n",
      "(array([-0.49914863,  0.01411715]), -1.0, False, {})\n",
      "(array([-0.41860276,  0.01704612]), -1.0, False, {})\n",
      "(array([-0.33239882,  0.01697563]), -1.0, False, {})\n",
      "(array([-0.25483218,  0.01422817]), -1.0, False, {})\n",
      "(array([-0.19701395,  0.00959519]), -1.0, False, {})\n",
      "(array([-0.1657919 ,  0.00392149]), -1.0, False, {})\n",
      "(array([-0.16426959, -0.00211762]), -1.0, False, {})\n",
      "(array([-0.19258694, -0.0079639 ]), -1.0, False, {})\n",
      "(array([-0.24796712, -0.01299252]), -1.0, False, {})\n",
      "(array([-0.32397854, -0.01640001]), -1.0, False, {})\n",
      "(array([-0.40992551, -0.01733887]), -1.0, False, {})\n",
      "(array([-0.49172331, -0.01530338]), -1.0, False, {})\n",
      "(array([-0.55474537, -0.01047678]), -1.0, False, {})\n",
      "(array([-0.58740244, -0.00372196]), -1.0, False, {})\n",
      "(array([-0.58371238,  0.00371146]), -1.0, False, {})\n",
      "(array([-0.54434798,  0.0104682 ]), -1.0, False, {})\n",
      "(array([-0.47653219,  0.01529831]), -1.0, False, {})\n",
      "(array([-0.39271135,  0.01733812]), -1.0, False, {})\n"
     ]
    }
   ],
   "source": [
    "# Initialize environment and reset it to start at valley with 0 velocity\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.reset() # array([-0.44943939,  0.        ])\n",
    "\n",
    "# experiment with constantly moving in one direction or the other to see what happens\n",
    "# constantly go right\n",
    "const_action = Push_Right\n",
    "for t in range(100):\n",
    "    update = env.step(const_action)\n",
    "    if t % 5 == 0:\n",
    "        print(update)\n",
    "    \n",
    "# if you constantly go right,  your position stays in (-0.52,-0.24)\n",
    "# your velocity stays in (-0.011,+0.011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-0.48093781, -0.00132897]), -1.0, False, {})\n",
      "(array([-0.50683037, -0.00763126]), -1.0, False, {})\n",
      "(array([-0.56044358, -0.01252674]), -1.0, False, {})\n",
      "(array([-0.63191597, -0.01512756]), -1.0, False, {})\n",
      "(array([-0.70844489, -0.01507362]), -1.0, False, {})\n",
      "(array([-0.77729269, -0.01261827]), -1.0, False, {})\n",
      "(array([-0.82834378, -0.00842423]), -1.0, False, {})\n",
      "(array([-0.8551691 , -0.00323992]), -1.0, False, {})\n",
      "(array([-0.85481448,  0.00228469]), -1.0, False, {})\n",
      "(array([-0.82731692,  0.00757176]), -1.0, False, {})\n",
      "(array([-0.77571274,  0.01199062]), -1.0, False, {})\n",
      "(array([-0.70652649,  0.01480313]), -1.0, False, {})\n",
      "(array([-0.6299614 ,  0.01530956]), -1.0, False, {})\n",
      "(array([-0.55879376,  0.01316345]), -1.0, False, {})\n",
      "(array([-0.50578696,  0.00861602]), -1.0, False, {})\n",
      "(array([-0.48069322,  0.0024797 ]), -1.0, False, {})\n",
      "(array([-0.48813244, -0.00411222]), -1.0, False, {})\n",
      "(array([-0.52673704, -0.0099479 ]), -1.0, False, {})\n",
      "(array([-0.58939177, -0.01394959]), -1.0, False, {})\n",
      "(array([-0.66464877, -0.01542744]), -1.0, False, {})\n"
     ]
    }
   ],
   "source": [
    "# Initialize environment and reset it to start at valley with 0 velocity\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.reset() # array([-0.44943939,  0.        ])\n",
    "\n",
    "# experiment with constantly moving in one direction or the other to see what happens\n",
    "# constantly go left\n",
    "const_action = Push_Left\n",
    "for t in range(100):\n",
    "    update = env.step(const_action)\n",
    "    if t % 5 == 0:\n",
    "        print(update)\n",
    "    \n",
    "# if you constantly go left,  your position stays in (-0.85,-0.48)\n",
    "# your velocity stays in (-0.015,+0.011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space:  Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "print('Action space: ', env.action_space)\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.2  -0.07] [0.6  0.07] [0.09  0.007] 1.8000000715255737 0.14000000432133675\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "To descretize the state space, we separate the range of possible continuous positions x and\n",
    "continuous velocities v into 20 bins (one_feature)\n",
    "'''\n",
    "\n",
    "n_feature_bins = 20 # number of state per one feature\n",
    "\n",
    "env_low = env.observation_space.low     \n",
    "env_high = env.observation_space.high   \n",
    "env_distance = (env_high - env_low) / n_feature_bins \n",
    "\n",
    "print(env_low, env_high, env_distance, env_distance[0]*20, env_distance[1]*20)\n",
    "# the range of x is 1.8 and velocity is 0.14 \n",
    "\n",
    "n_states = n_feature_bins*n_feature_bins # states are the space of combinations of x and v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the update, the tuple  (array([-0.66511328, -0.01531442]), -1.0, False, {})\n",
    " contains the (state, reward, end_of_episode_bool, meta_data)\n",
    "the meta_data is empty for this environment, there is a negative reward for every timestep that \n",
    " you have not reached the goal yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 130, 3)\n",
      "[-0.90691623 -0.02983074  0.        ] (3,)\n"
     ]
    }
   ],
   "source": [
    "# load the expert 20 demonstrations\n",
    "#expert_demo = np.load('/shared/Carson/RL/lets-do-irl/mountaincar/maxent/expert_demo/expert_demo.npy')\n",
    "expert_demo = np.load('Data/expert_demo.npy')\n",
    "print(expert_demo.shape)\n",
    "# (number of demonstrations, length of demonstrations, states and actions of demonstrations)\n",
    "print(expert_demo[0,60,:], expert_demo[0,0,:].shape)\n",
    "# as you can see from step 60 of the first example, the best strategy is to first accelerate backwards into\n",
    "# the < -0.8 range into order to gain speed going right into the valley. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "using this bin size we descretize the expert demonstrations\n",
    "The difference between expert_demo and demonstrations is that\n",
    "demonstrations consists of discreet integers instead of continuous values\n",
    "for it's states\n",
    "\n",
    "\n",
    "\n",
    "here I used n for sample index and t for timestep index\n",
    "'''\n",
    "def idx_state(state):\n",
    "    '''\n",
    "    this function converts a continuous state vector of\n",
    "    2-dim into a discrete index\n",
    "    \n",
    "    by assigning each state and index of state_idx = position_idx + velocity_idx * one_feature\n",
    "    we make sure that (position_idx, velocity_idx) = (2,3) and (3,2) map to different integers\n",
    "    The first 20 elements of state_idx go to position_idx = 0 - 19, velocity_idx = 0, etc\n",
    "    '''\n",
    "    position_idx = int((state[0] - env_low[0]) / env_distance[0])\n",
    "    velocity_idx = int((state[1] - env_low[1]) / env_distance[1])\n",
    "    state_idx = position_idx + velocity_idx * n_feature_bins\n",
    "    return state_idx\n",
    "\n",
    "demonstrations = np.zeros((len(expert_demo), len(expert_demo[0]), 3))\n",
    "\n",
    "for n in range(len(expert_demo)):\n",
    "    \n",
    "    for t in range(len(expert_demo[0])):\n",
    "\n",
    "        state_idx = idx_state(expert_demo[n][t])\n",
    "\n",
    "        demonstrations[n][t][0] = state_idx\n",
    "        demonstrations[n][t][1] = expert_demo[n][t][2] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning is a ‚Äúmodel-free, off-policy‚Äù RL algorithm.\n",
    "\n",
    "‚Äúmodel-free‚Äù because we are not trying to build a pre-determined model of our environment (which may or may not be accurate)\n",
    "\n",
    "‚Äúoff-policy‚Äù because our behavior in interacting with (or exploring) the environment may be unrelated to what we believe to be the best optimal at the time of taking the action.\n",
    "\n",
    "You might think of Q-learning as updating a table where each row is a different state and each column is an action that can be taken in that state and where they intersect is the Q-value for that state action pair. the Q-table maps various (state, action) pairs with the Q-value (the expected sum of discounter future rewards when taking action a in state s and behaving optimally thereafter)\n",
    "\n",
    "It doesnt have to be a square table like this though, Q just has to map some state or state-action to some Q-value\n",
    "\n",
    "In the above example you can see that the pattern is that we observe some state, s, take an action a, and receive a new state s‚Äô, then repeat. \n",
    "\n",
    "The update rule is the weighted average of the old Q(s,a) value and the Q value implied by the new observation. That is, the sum of (i) the immediate reward and (ii) the expected discounted reward received from the new state onwards, assuming you always choose the optimal action.\n",
    "\n",
    "Q‚Äô(s,a) = (1 ‚Äî alpha) * Q(s, a) + alpha *(r + gamma * Q(s‚Äô, argmax a‚Äô : Q(s‚Äô, a‚Äô)))\n",
    "\n",
    "where argmax a‚Äô : Q(s‚Äô, a‚Äô) is the action  a‚Äô that has the highest current Q value among our row of states s‚Äô\n",
    "\n",
    "This equation for update is equivalently written \n",
    "\n",
    "ùëÑ(ùë†,ùëé)‚ÜêùëÑ(ùë†,ùëé)+ùõº(ùëü+ùõæ*max_ùëé‚Ä≤ùëÑ(ùë†‚Ä≤,ùëé‚Ä≤)‚àíùëÑ(ùë†,ùëé)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.zeros((n_states, n_actions)) # (400, 3)\n",
    "\n",
    "def update_q_table(state, action, reward, q_learning_rate, gamma, next_state):\n",
    "    ''' \n",
    "    The Q-learning update rule\n",
    "    ùëÑ(ùë†,ùëé)‚ÜêùëÑ(ùë†,ùëé)+ùõº(ùëü+ùõæ*max_ùëé‚Ä≤ùëÑ(ùë†‚Ä≤,ùëé‚Ä≤)‚àíùëÑ(ùë†,ùëé))\n",
    "    ùõæ or gamma, is the discount factor\n",
    "    ùõº or alpha, is the q_learning_rate\n",
    "    '''\n",
    "    q_1 = q_table[state][action]\n",
    "    #print(reward, gamma, max(q_table[next_state])) \n",
    "    q_2 = reward + gamma * max(q_table[next_state])\n",
    "    q_table[state][action] += q_learning_rate * (q_2 - q_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Inverse Reinforcement Learning (IRL)\n",
    "\n",
    "In IRL, instead of randomly exploring the world learning by trial and error, we want to approximate the reward function that experts are using by observing expert demonstrations. This reward function is parameterized by theta\n",
    "\n",
    "if $f_si$ is the feature vector of some state_i, then the reward = theta.dot(f_si)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(feature_matrix, theta, n_states, state_idx):\n",
    "    \n",
    "    '''\n",
    "    theta is the same length as the number of states, so this\n",
    "    just a list of the rewards associated with being in each \n",
    "    state and the function here simply selects the\n",
    "    element in theta that corresponds to the sate\n",
    "    '''\n",
    "    \n",
    "    irl_rewards = feature_matrix.dot(theta).reshape((n_states,))\n",
    "    \n",
    "    return irl_rewards[state_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the Objective Function in MaxEntIRL?\n",
    "\n",
    "Maximizing the entropy of the distribution over paths subject to the feature constraints from observed data implies that we maximize the likelihood of the observed data under the maximum entropy (exponential family) distribution.\n",
    "\n",
    "The Product of Probabilites vs The Log Sum of Probabilities\n",
    "\n",
    "$\n",
    "\\log \\left(\\prod_i P(x_i)\\right) = \\sum_i \\log \\left( P(x_i)\\right)\n",
    "$\n",
    "\n",
    "the logarithm is a monotonic transformation that preserves the locations of the extrema, in particular, the estimated parameters in maximum-likelihood are identical for the original and the log-transformed formulation, while also morenumerically stable and symbolically easier to differentiate than the former since the sum rule is easier than the product rule for differentiation\n",
    "\n",
    "Find the Reward Parameters that maximize the likelihood of the expert trajectories, theta_star\n",
    "\n",
    "$ \n",
    "\\theta^{*} = \\underset{\\theta}{\\arg\\max} \\underset{demonstrations}\\prod[P(trajectories|\\theta)] = \\underset{\\theta}{\\arg\\max} \\underset{demonstrations}\\sum log[P(trajectories|\\theta)] = \\underset{\\theta}{\\arg\\max} L(\\theta)\n",
    "$\n",
    "\n",
    "The gradient for this Loss function L is\n",
    "\n",
    "$\\nabla L(\\theta) = f_{expert} - \\underset{trajectories}\\sum P(trajectory|\\theta)f_{learner}\n",
    "= f_{expert} - \\underset{states}\\sum D_{s} f_{s}\n",
    "$\n",
    "\n",
    "Where D_s is the expected state visitation frequencies\n",
    "\n",
    "gradient = expected_expert_feats - expected_learner_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_theta(expert, learner, theta, theta_learning_rate):\n",
    "    '''\n",
    "    The gradient is the difference between expected\n",
    "    empirical feature counts and the learner‚Äôs expected feature\n",
    "    counts, which can be expressed in terms of expected state\n",
    "    visitation frequencies, D_si\n",
    "    \n",
    "    the two lines below apply these two formulas\n",
    "    \n",
    "    ‚àáùêø(ùúÉ)=ùëì_ùëíùë•ùëùùëíùëüùë°‚àí‚àëùë°ùëüùëéùëóùëíùëêùë°ùëúùëüùëñùëíùë†ùëÉ(ùë°ùëüùëéùëóùëíùëêùë°ùëúùëüùë¶|ùúÉ)ùëì_ùëôùëíùëéùëüùëõùëíùëü=ùëì_ùëíùë•ùëùùëíùëüùë°‚àí‚àëùë†ùë°ùëéùë°ùëíùë†ùê∑ùë†ùëìùë†\n",
    "    \n",
    "    ùúÉ := ùúÉ + ùõº‚àáùêø(ùúÉ)\n",
    "    \n",
    "    where ùõº is the theta_learning_rate, not the q-learning learning rate\n",
    "    '''\n",
    "    gradient = expert - learner\n",
    "    theta += theta_learning_rate * gradient\n",
    "\n",
    "    # Clip theta\n",
    "    for j in range(len(theta)):\n",
    "        if theta[j] > 0:\n",
    "            theta[j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "feature_matrix = np.eye((n_states)) # (400, 400)\n",
    "\n",
    "print(feature_matrix)\n",
    "\n",
    "def get_expert_feature_expectations(feature_matrix, demonstrations):\n",
    "    '''\n",
    "    f_tilda = (1/m)*sum all trajectory's feature count\n",
    "    Since our states are encoded as indices for 400 possible states, \n",
    "    this function adds together one-hot encodings of\n",
    "    every state visited by each expert demonstration throughout the\n",
    "    example trajectory and at the end divides the total number of \n",
    "    example trajectories (expert demonstrations)\n",
    "    \n",
    "    for each 400 states indices, aka each (x,v) pairs, \n",
    "    value of 0.5 at that index means that 50% of all trajectories have this state\n",
    "    '''\n",
    "    feature_expectations = np.zeros(feature_matrix.shape[0])\n",
    "    \n",
    "    for demonstration in demonstrations:\n",
    "        for state_idx, _, _ in demonstration:\n",
    "            feature_expectations += feature_matrix[int(state_idx)]\n",
    "\n",
    "    feature_expectations /= demonstrations.shape[0]\n",
    "    return feature_expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.17022005e-01 -7.20324493e-01 -1.14374817e-04 -3.02332573e-01\n",
      " -1.46755891e-01 -9.23385948e-02 -1.86260211e-01 -3.45560727e-01] (400,)\n",
      "[0.8  0.65 0.45 0.1  0.   0.   0.   0.   0.   3.4 ] (400,)\n",
      "0 episode score is -200.00\n",
      "1000 episode score is -183.53\n",
      "2000 episode score is -182.77\n",
      "3000 episode score is -177.25\n",
      "4000 episode score is -177.33\n",
      "5000 episode score is -176.36\n",
      "6000 episode score is -175.94\n",
      "7000 episode score is -174.84\n",
      "8000 episode score is -174.34\n",
      "9000 episode score is -173.21\n",
      "10000 episode score is -171.69\n",
      "11000 episode score is -170.09\n",
      "12000 episode score is -168.73\n",
      "13000 episode score is -167.53\n",
      "14000 episode score is -166.52\n",
      "15000 episode score is -165.66\n",
      "16000 episode score is -164.90\n",
      "17000 episode score is -164.22\n",
      "18000 episode score is -163.63\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "'''\n",
    "Theta Œ∏ is the vector containing the reward weights. \n",
    "reward(state_t) = np.dot(Œ∏,f(state_t)), where f is the feature vector of state_t\n",
    "'''\n",
    "theta_learning_rate = 0.05\n",
    "theta = -(np.random.uniform(size=(n_states,)))\n",
    "print(theta[:8], theta.shape)\n",
    "gamma = 0.99\n",
    "q_learning_rate = 0.03\n",
    "\n",
    "expected_expert_feats = get_expert_feature_expectations(feature_matrix, demonstrations) # (400,)\n",
    "print(expected_expert_feats[370:380], expected_expert_feats.shape)\n",
    "\n",
    "'''\n",
    "initialize the learner_feature_counts that when divided by the number of updates \n",
    "becomes he\n",
    "'''\n",
    "learner_feature_counts = np.zeros(n_states)\n",
    "\n",
    "episodes, scores = [], []\n",
    "\n",
    "for episode in range(30000):\n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "\n",
    "    if (episode != 0 and episode == 10000) or (episode > 10000 and episode % 5000 == 0):\n",
    "        \n",
    "        expected_expert_feats = learner_feature_counts / episode\n",
    "        \n",
    "        update_theta(expected_expert_feats, expected_expert_feats, theta, theta_learning_rate)\n",
    "\n",
    "    while True:\n",
    "        state_idx = idx_state(state)\n",
    "        action = np.argmax(q_table[state_idx])\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        irl_reward = get_reward(feature_matrix, theta, n_states, state_idx)\n",
    "        next_state_idx = idx_state(next_state)\n",
    "        update_q_table(state_idx, action, irl_reward, q_learning_rate, gamma, next_state_idx)\n",
    "\n",
    "        learner_feature_counts += feature_matrix[int(state_idx)]\n",
    "\n",
    "        score += reward\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            scores.append(score)\n",
    "            episodes.append(episode)\n",
    "            break\n",
    "\n",
    "    if episode % 1000 == 0:\n",
    "        score_avg = np.mean(scores)\n",
    "        print('{} episode score is {:.2f}'.format(episode, score_avg))\n",
    "        pylab.plot(episodes, scores, 'b')\n",
    "        pylab.savefig(\"Data/maxent_30000.png\")\n",
    "        np.save(\"Data/maxent_q_table\", arr=q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
